{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1S7d7_foHPJb"
   },
   "source": [
    "# Week 7 Applying Machine Learning\n",
    "\n",
    "So far you have seen several different types of models. In the main module this week, you will be challenged to solve a problem using any of the tools you have learned in this course. The purpose of this pre-module is to introduce you to two more models that you may find useful in tackling this challenge: SVMs and XGBoost. While in this module we will not be covering the internal details of how each model works, we will provide enough detail regarding the hyperparameters so that you can use them on your dataset.\n",
    "\n",
    "As usual, we will use the Pima Indian Diabetes dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ixc0OuOuwvcB"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# we fetch the dataset from https://www.openml.org/search?type=data&status=active&id=37\n",
    "X,y = fetch_openml(data_id = 37, as_frame = True, return_X_y = True)\n",
    "\n",
    "# convert tested_positive and tested_negative to 1 and 0\n",
    "y = (y == 'tested_positive').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZ_0bzwY2PPF"
   },
   "source": [
    "---\n",
    "\n",
    "##### **Q1: Split the data into a train and test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MySPnKni2PV2"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "### Your Code Here.\n",
    "## NOTE: please use X_train, X_test, y_train, and y_test as your variable names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8KOJuAi2Wrg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvSKPFD7DzIk"
   },
   "source": [
    "## Support Vector Machines (SVMs)\n",
    "\n",
    "In Week 5, you explored the basics of machine learning models like Logistic Regression. This week, we will introduce a new model: **Support Vector Machines (SVMs)**.\n",
    "\n",
    "SVMs are supervised learning models used for **classification** and **regression** tasks. They work by finding **hyperplanes** (AKA multi-dimensional lines) that best separates data points from different classes in a dataset. For example, in a biological context, an SVM could classify whether a gene is active or inactive based on certain gene expression features.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Definitions:\n",
    "\n",
    "- **Hyperplane**: A hyperplane is the decision boundary that separates different classes. For a 2D dataset, it is a line; for a 3D dataset, it is a plane; and in higher dimensions, it is generalized to a hyperplane.  \n",
    "- **Support Vectors**: These are the data points closest to the hyperplane. They “support” the hyperplane and play a crucial role in defining its position.  \n",
    "- **Margin**: The margin is the distance between the hyperplane and the closest data points from each class. SVMs aim to maximize this margin for better generalization.\n",
    "\n",
    "![hyperplane](hpplane.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCM8Hex2HLI-"
   },
   "source": [
    "\n",
    "### Kernel Trick for Non-Linear Data\n",
    "\n",
    "As you well know by now, not all data can be cleanly separated by a straight line. SVMs use functions known as kernels to project data into a higher-dimensional space where it becomes separable. A common kernel is the Radial Basis Function (RBF). This is effectively equal to adding extra features to the data to make it easier to seperate different classes. The most common kernel is the *radial basis function.* This is effectively adding a feature that is equivalent to how far the point is from the center of a circle, allowing the model to seperate classes that are not seperable by a hyperplane (straight line). \n",
    "\n",
    "![kernel_trick](kernel_Trick.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1TcVmgdxFbw"
   },
   "source": [
    "### Parameters to Tune\n",
    "\n",
    "The SVM has 3 main parameters one can tune:\n",
    "1. `max_iter`: Like Logistic Regression, this is the number of training iterations the SVM will do.\n",
    "2. `kernel`:  This parameter controls what kernel we use. Often we set this to `linear`, `rbf` (radial basis function), or `poly` (adding squared and cubic terms).\n",
    "3. `C`: This parameter should be set between zero and 1 and controls how much the hyperplane depends on the features, with higher values of C allowing the hyperplane to use more features (smaller feature coefficients). Often if your model is overfitting, setting `C` to a lower value may help as it will make the hyperplane more robust to noisy features.\n",
    "\n",
    "Below we provide an example of training an SVM on the diabetes dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhJOGdpOzxiZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(max_iter=1000, # 1000 iterations\n",
    "            kernel='rbf', # Radial Basis Kernel\n",
    "            C=1.0) # do not limit hyperplane by much\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzzaa10i4PNC"
   },
   "source": [
    "---\n",
    "\n",
    "##### **Q2: Make predictions on the train and test set, and provide the training and testing accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmH7MuNr4Pkd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETqpsVPU4Poj"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQw4zqm0GHCr"
   },
   "source": [
    "\n",
    "## XGBoost\n",
    "\n",
    "In week 6, you learned about decision trees. This week, we will introduce **XGBoost**, which uses decision trees as a foundation to build a much more robust model architecture.\n",
    "\n",
    "XGBoost, short for e**X**treme **G**radient **Boost**ing, is a supervised learning algorithm that builds a strong prediction model by combining many weak models (typically decision trees). It is widely used for structured/tabular data in both **classification** and **regression** tasks.\n",
    "\n",
    "\n",
    "### Boosting\n",
    "\n",
    "XGBoost functions by training *multiple decision trees.* To put it simply, imagine you are playing darts and are trying to hit a target. With just one throw, you are very likely to miss the target. But if you had a second throw, you could adjust based on how much you missed the first throw and you're more likely to hit it.\n",
    "\n",
    "XGBoost works in a similar way. It first trains one decision tree, makes predictions on the training set, and then trains another tree to \"fix\" and adjust the predictions. We then get the combined predictions from the two trees, and identify where the two trees got the prediction wrong, and train another tree to fix the errors. We then keep repeating this until we can't fix any more errors.\n",
    "\n",
    "![training](boosting.png)\n",
    "---\n",
    "\n",
    "### Key Definitions:\n",
    "\n",
    "- **Boosting**: Boosting is an ensemble method that combines the predictions of several weak models (e.g., shallow decision trees) to form a strong model.  \n",
    "- **Learning Rate**: This parameter controls how much each tree contributes to the final model/how much each tree adjusts the predictions of the previous tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zMW9blp9ORl"
   },
   "source": [
    "### Parameters to Tune\n",
    "\n",
    "Some common parameters tuned by XGBoost are:\n",
    "1. `n_estimators`: This is the max number of trees we will construct.\n",
    "2. `max_depth`: This controls the maximum depth of each individual tree.\n",
    "3. `learning_rate`: As mentioned above, this parameter controls how much each tree updates the predictions of the previous trees.\n",
    "\n",
    "Below we provide an example of training a XGBoost on the diabetes dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7I0ZNJBt9Nxk"
   },
   "outputs": [],
   "source": [
    "# we often first need to install xgboost,\n",
    "# which we will do with the command below\n",
    "!pip install xgboost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "        n_estimators = 100, # 100 trees\n",
    "        max_depth=2, # each tree only has a depth of 2\n",
    "        learning_rate=0.1, # update scale\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2S9MaTnA9om"
   },
   "source": [
    "---\n",
    "\n",
    "##### **Q3: Make predictions on the train and test set using XGBoost, and provide the training and testing accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlnXu_CEA9Um"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L54BH3WdBlsk"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH4zdxWdHX_1"
   },
   "source": [
    "## Summary\n",
    "This week, you learned about two new models, SVMs and XGBoost. These are powerful, state-of-the-art machine learning models. In the next module, you will have an  You gained practical experience in training, evaluating, and interpreting these models. In the next module, you'll apply them to solve a biological case study problem by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_yD8uwjG__y"
   },
   "source": [
    "### Graded Question:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bunFP_LdB9g3"
   },
   "source": [
    "\n",
    "##### **GQ1: Did the SVM overfit or underfit (1pt)? What about XGBoost (1pt)? Justify your reasoning (2pts)**\n",
    "\n",
    "*Your Answer Here*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
