{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1S7d7_foHPJb"
   },
   "source": [
    "# Week 7 Applying Machine Learning\n",
    "\n",
    "Thus far, you have been introduced to various types of models. In this week's main module, you will be tasked with solving a problem using any of the tools you have learned so far. This pre-module aims to introduce you to two additional models that may be useful in tackling this challenge: Support Vector Machines (SVMs) and extreme Gradient Boosting (XGBoost). While we will not delve into the details of each model, or how it works, we will provide enough details on their hyperparameters to enable you to apply them to your dataset.\n",
    "\n",
    "As usual, we will be using the Pima Indian Diabetes dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ixc0OuOuwvcB"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# we fetch the dataset from https://www.openml.org/search?type=data&status=active&id=37\n",
    "X,y = fetch_openml(data_id = 37, as_frame = True, return_X_y = True)\n",
    "\n",
    "# convert tested_positive and tested_negative to 1 and 0\n",
    "y = (y == 'tested_positive').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZ_0bzwY2PPF"
   },
   "source": [
    "---\n",
    "\n",
    "##### **Q1: Split the data into a train and test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MySPnKni2PV2"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "### Your Code Here.\n",
    "## NOTE: please use X_train, X_test, y_train, and y_test as your variable names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8KOJuAi2Wrg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvSKPFD7DzIk"
   },
   "source": [
    "## Support Vector Machines (SVMs)\n",
    "\n",
    "In Week 5, you explored the basics of machine learning models like Logistic Regression. This week, we will introduce a new model: **Support Vector Machines (SVMs)**.\n",
    "\n",
    "SVMs are supervised learning models used for **classification** and **regression** tasks. They work by finding **hyperplanes** (AKA multi-dimensional lines) that best separates data points from different classes in a dataset. For example, in a biological context, an SVM could classify whether a gene is active or inactive based on certain gene expression features.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Definitions:\n",
    "\n",
    "- **Hyperplane**: A hyperplane is the decision boundary that separates different classes. For a 2D dataset, it is a line; for a 3D dataset, it is a plane; and in higher dimensions, it is generalized to a hyperplane.  \n",
    "- **Support Vectors**: These are the data points closest to the hyperplane. They “support” the hyperplane and play a crucial role in defining its position.  \n",
    "- **Margin**: The margin is the distance between the hyperplane and the closest data points from each class. SVMs aim to maximize this margin for better generalization.\n",
    "\n",
    "![hyperplane](hpplane.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCM8Hex2HLI-"
   },
   "source": [
    "\n",
    "### Kernel Trick for Non-Linear Data\n",
    "\n",
    "As you well know by now, not all data can be neatly separated by a straight line. SVMs employ functions known as kernels to project data into a higher-dimensional space, where it becomes separable. One commonly used kernel is the Radial Basis Function (RBF). This kernel effectively adds extra features to the data, facilitating the separation of different classes. More specifically, the RBF kernel adds a feature equivalent to the distance of a point from the centre of a circle, enabling the model to separate classes that are not linearly separable.\n",
    "\n",
    "![kernel_trick](kernel_Trick.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1TcVmgdxFbw"
   },
   "source": [
    "### Parameters to Tune\n",
    "\n",
    "The SVM has three primary parameters that can be tuned:\n",
    "1. `max_iter`: Similar to Logistic Regression, this parameter specifies the number of training iterations the SVM will perform.\n",
    "2. `kernel`:  This parameter determines the kernel function to be used. Common choices include `linear`, radial basis function `rbf` radial basis function, or `poly` (which adds squared and cubic terms).\n",
    "3. `C`: This parameter, which should be set between zero and one, controls the extent to which the hyperplane depends on the features. Higher values of C allow the hyperplane to utilize more features (resulting in smaller feature coefficients). If your model is overfitting, reducing the value of C may help by making the hyperplane more robust to noisy features.\n",
    "\n",
    "Below, we provide an example of training an SVM on the diabetes dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhJOGdpOzxiZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = SVC(max_iter=1000, # 1000 iterations\n",
    "            kernel='rbf', # Radial Basis Kernel\n",
    "            C=1.0) # do not limit hyperplane by much\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzzaa10i4PNC"
   },
   "source": [
    "---\n",
    "\n",
    "##### **Q2: Make predictions on the train and test set, and provide the training and testing accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmH7MuNr4Pkd"
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "train_predictions = ... # model.predict()\n",
    "test_predictions = ...\n",
    "\n",
    "# determine accuracy\n",
    "train_accuracy = ... # accuracy_score()\n",
    "test_accuracy = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETqpsVPU4Poj"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQw4zqm0GHCr"
   },
   "source": [
    "\n",
    "## XGBoost\n",
    "\n",
    "In week 6, you learned about decision trees. This week, we will introduce **XGBoost**, which uses decision trees as a foundation to build a much more robust model architecture.\n",
    "\n",
    "XGBoost, short for e**X**treme **G**radient **Boost**ing, is a supervised learning algorithm that builds a strong prediction model by combining many weak models (typically decision trees). It is widely used for structured/tabular data in both **classification** and **regression** tasks.\n",
    "\n",
    "\n",
    "### Boosting\n",
    "\n",
    "XGBoost operates by training multiple decision trees. To illustrate, imagine playing darts and aiming for a target. With a single throw, you are likely to miss the target. However, with a second throw, you can adjust based on the initial miss, increasing your chances of hitting the target.\n",
    "\n",
    "XGBoost functions similarly. It begins by training one decision tree and making predictions on the training set. Subsequently, it trains another tree to correct and adjust these predictions. The combined predictions from the two trees are then analyzed to identify errors, and another tree is trained to address these errors. This process is repeated iteratively until no further errors can be fixed.\n",
    "\n",
    "\n",
    "![training](boosting.png)\n",
    "---\n",
    "\n",
    "### Key Definitions:\n",
    "\n",
    "- **Boosting**: Boosting is an ensemble method that combines the predictions of several weak models (e.g., shallow decision trees) to form a strong model.  \n",
    "- **Learning Rate**: This parameter determines the extent to which each tree contributes to the final model and adjusts the predictions of the preceding tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zMW9blp9ORl"
   },
   "source": [
    "### Parameters to Tune\n",
    "\n",
    "Some common parameters tuned by XGBoost are:\n",
    "1. `n_estimators`: The max number of trees we will construct.\n",
    "2. `max_depth`: Controls the maximum depth of each individual tree.\n",
    "3. `learning_rate`: As mentioned above, this parameter controls how much each tree updates the predictions of the previous trees.\n",
    "\n",
    "Below we provide an example of training a XGBoost on the diabetes dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7I0ZNJBt9Nxk"
   },
   "outputs": [],
   "source": [
    "# we often first need to install xgboost,\n",
    "# which we will do with the command below\n",
    "!pip install xgboost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "        n_estimators = 100, # 100 trees\n",
    "        max_depth=2, # each tree only has a depth of 2\n",
    "        learning_rate=0.1, # update scale\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2S9MaTnA9om"
   },
   "source": [
    "---\n",
    "\n",
    "##### **Q3: Make predictions on the train and test set using XGBoost, and provide the training and testing accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlnXu_CEA9Um"
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "train_predictions = ... # xgb.predict()\n",
    "test_predictions = ...\n",
    "\n",
    "# determine accuracy\n",
    "train_accuracy = ... # accuracy_score()\n",
    "test_accuracy = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L54BH3WdBlsk"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_yD8uwjG__y"
   },
   "source": [
    "### Graded Question:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bunFP_LdB9g3"
   },
   "source": [
    "\n",
    "##### **GQ1: Compare the performance of the decision tree model created in the first half of this module with the model of your choice developed in the second half. Which model performed better? (1 mark) Justify your reasoning using model evaluation metrics (1 mark) and explain the reasons for any observed differences (1 mark).**\n",
    "\n",
    "*Your Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This week, you were introduced to two advanced machine learning models: SVMs and XGBoost. These are powerful, state-of-the-art machine learning models. You gained practical experience in training, evaluating, and interpreting these powerful models. In the next module, you will apply them independently to solve a biological case study problem."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
